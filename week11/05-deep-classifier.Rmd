---
title: "Recurrent network with GRU layer and pretrained GloVe embeddings"
author: "Friedrich Geiecke"
date: "28 March 2022"
output: html_document
---

--------------------------------------------------------------------------------

#### Preliminary remarks

To install `keras` and run the notebooks from this week:

1. Run `install.packages("keras")`
2. Load the package with `library("keras")`
3. Then run the function `install_keras()` in the R-console

Afterwards the code in this notebook should run properly.

For more information on `tensorflow` (the basis of `keras`) see: https://tensorflow.rstudio.com/installation/

--------------------------------------------------------------------------------

This notebook serves as an outlook to illustrate that deep learning based sentiment classification can relatively easily be implemented in R. We will use TensorFlow/Keras and slightly modify the code from this sample https://keras.rstudio.com/articles/examples/pretrained_word_embeddings.html such that it can solve our problem. This model will use pretrained GloVe word embeddings for each word in the input sentence and thereby also connect the topic to our discussion of word embeddings last week.

A great repo with many simple Keras R code samples for deep learning (and sentiment classification) models can be found here https://github.com/rstudio/keras/tree/master/vignettes/examples (also the one on which this notebook is built).

Loading packages:

```{r}
library("keras")
library("tensorflow")
library("tidyverse")
library("tm")

# TensorFlow/Keras specific pseudo random number seed
set_random_seed(24)
```


Loading the data:

```{r}
df <- read.csv("labeled_tweets_processed.csv")
```

Creating a training / test split which is equal for all classifiers:

```{r}
set.seed(24)
test_fraction <- 0.3
training_indices <- sample(1:nrow(df), floor(nrow(df)*(1-test_fraction)))
```

Loading GloVe word embeddings:

```{r}
embeddings <- data.table::fread('glove.6B.300d.txt', data.table = FALSE,  encoding = 'UTF-8')
colnames(embeddings) <- c('word', paste('dim',1:300,sep = '_'))
```

Setting model parameters:

```{r}
max_vocabulary <- 10000
max_words_per_document <- 60
dim_size <- 300
```

Pre-processing data and creating embeddings for the model:

```{r}
# Creating a fitted tokenizer object
tokenizer_fitted <- text_tokenizer(num_words = max_vocabulary) %>%
  fit_text_tokenizer(df$text)

# Obtain indices for each word and combine into training and test sets
# Pad all documents to a maximum number of words
training_X <- texts_to_sequences(tokenizer_fitted, df$text[training_indices]) %>%
  pad_sequences(maxlen = max_words_per_document)

test_X <- texts_to_sequences(tokenizer_fitted, df$text[-training_indices]) %>%
  pad_sequences(maxlen = max_words_per_document)

# Encoding of y
y <- df$label
y[y == "disapprove"] <- "0"
y[y == "neutral"] <- "1"
y[y == "approve"] <- "2"
y <- as.numeric(y)
class_names <- c("disapprove", "neutral", "approve")

training_y <- y[training_indices]
test_y <- y[-training_indices]

# unlist word indices
word_indices = unlist(tokenizer_fitted$word_index)

# Creating a dataframe with word indices
dic <- data.frame(word = names(word_indices), key = word_indices, stringsAsFactors = FALSE) %>%
  arrange(key) %>% .[1:max_vocabulary,]

# For the final embedding table, join the words with GloVe vectors and
# if word does not exist in GloVe, then fill NA's with 0
embeddings_for_model <- dic  %>% left_join(embeddings) %>% .[,3:302] %>% replace(., is.na(.), 0) %>% as.matrix()
```

Building the model (a bidirectional RNN with GRU layer):

```{r}
input <- layer_input(shape = list(max_words_per_document), name = "input")

model <- input %>%
  layer_embedding(input_dim = max_vocabulary, output_dim = dim_size, input_length = max_words_per_document,
                  weights = list(embeddings_for_model), trainable = FALSE) %>% # note: embeddings could also be further trained
  layer_spatial_dropout_1d(rate = 0.2 ) %>%
  bidirectional(
    layer_gru(units = 80, return_sequences = TRUE)
  )
max_pool <- model %>% layer_global_max_pooling_1d()
ave_pool <- model %>% layer_global_average_pooling_1d()

output <- layer_concatenate(list(ave_pool, max_pool)) %>%
  layer_dense(3, activation = "softmax")
  
model <- keras_model(input, output)
```


Compiling the model (adding a loss, optimiser and evaluation metric):

```{r}
model %>% compile(
    loss = "sparse_categorical_crossentropy",
    optimizer = "adam",
    metrics = "accuracy"
)
```

Training:

```{r}
history <- model %>% keras::fit(
  training_X, training_y,
  epochs = 6,
  batch_size = 64,
  validation_split = 0.2
)
```

Evaluation:

```{r}
model %>% evaluate(test_X, test_y, verbose = 0)
```

Even with minimal tuning and on a very small dataset, it achieves roughly the same accuracy as the other models.


Generic function to predict approval and disapproval from raw sentences, now adjusted to Keras model:

```{r}
approve_or_dissapprove <- function(sentence,
                                   tf_model = model,
                                   tf_tokenizer = tokenizer_fitted,
                                   tf_max_words_per_document = max_words_per_document,
                                   outcome_class_names = class_names) {
  
  # Cleaning text before passing it through the dfm matrix
  
  # Convert to lower case
  sentence <- sentence %>% tolower()
  
  # Remove Twitter handles and hashtags
  sentence <- str_replace_all(sentence, pattern = "[@#]\\S+", replacement = "")
  
  # Remove URLs
  sentence <- str_replace_all(sentence, pattern = "(http|www)\\S+", replacement = "")
  
  # Get rid of non ASCII chracters (largely emojis in this case)
  sentence <-gsub("[^\x01-\x7F]", "", sentence)
  
  # Remove punctuation, numbers, and excess white spaces within the texts and at their beginning/end
  sentence <- sentence %>% removePunctuation() %>%
    removeNumbers() %>% stripWhitespace() %>% trimws()
  
  
  # Transform sentence
  sentence_X <- texts_to_sequences(tf_tokenizer, sentence) %>%
  pad_sequences(maxlen = tf_max_words_per_document)

  probabilities <- model %>% predict(sentence_X)
  
  predicted_label <- outcome_class_names[which.max(probabilities)]
  
  return(predicted_label)
  
}
```

This model indeed seems to better understand the interdependence of words:

```{r}
approve_or_dissapprove("This policy is great.")
approve_or_dissapprove("This policy is not great.")

approve_or_dissapprove("This policy is great ... not")
```

Back to our previous examples:

```{r}

# Neutral
approve_or_dissapprove("The last week of lent term begins.")

# Approval
approve_or_dissapprove("The government is doing a great job.")

# Disapproval
approve_or_dissapprove("I think this is a bad policy.")
approve_or_dissapprove("The government is doing a bad job.")
approve_or_dissapprove("I dont think this is a good policy.")

# Examples of what it misses
approve_or_dissapprove("I think this is a good policy.")
approve_or_dissapprove("This is a course in textual analysis.")
approve_or_dissapprove("I do not think this is a good policy.")
approve_or_dissapprove("I think this is a valuable policy.")
```


While we see some evidence of the model understanding interdependencies beyond the level of the previous models, other predictions are still far off. Note that to obtain competitive performance with such a model, we would have to spend time tuning the hyperparameters and architecture, and use a much larger training dataset than only around 25,000 observations.

References

- https://keras.rstudio.com/articles/examples/pretrained_word_embeddings.html
---
title: "Pre-processing tweets"
author: "Friedrich Geiecke"
date: "28 March 2022"
output: html_document
---

In this notebook, we are going to process the downloaded Twitter data in order to generate a training dataset for our classifiers.

Loading packages:

```{r}
library("tm")
library("tidyverse")
library("rtweet")
library("quanteda")

# Function which allows to recover corrupted tweet jsons returned by rtweet
# Discussion: https://github.com/ropensci/rtweet/issues/356
source("https://gist.githubusercontent.com/JBGruber/dee4c44e7d38d537426f57ba1e4f84ab/raw/ce28d3e8115f9272db867158794bc710e8e28ee5/recover_stream.R")
```

Emojis of interest in the example we consider:

```{r}
emojis_class_1 <- c("\U0001F44E","\U0001F44E\U0001F3FB", "\U0001F44E\U0001F3FC", "\U0001F44E\U0001F3FD", "\U0001F44E\U0001F3FE", "\U0001F44E\U0001F3FF")
emojis_class_2 <- c("\U0001F44D","\U0001F44D\U0001F3FB", "\U0001F44D\U0001F3FC", "\U0001F44D\U0001F3FD", "\U0001F44D\U0001F3FE", "\U0001F44D\U0001F3FF", "\U0001F44F", "\U0001F44F\U0001F3FB", "\U0001F44F\U0001F3FC", "\U0001F44F\U0001F3FD", "\U0001F44F\U0001F3FE", "\U0001F44F\U0001F3FF")

label_0 = "neutral"
label_1 = "disapprove"
label_2 = "approve"

cat(emojis_class_1)
cat("\n")
cat(emojis_class_2)
```

Next, let us write a function to load all tweet json file and concatenate them into one dataframe. A current issue of the `rtweet` package is that the `stream_tweets` function can yield corrupted json files, which then are impossible to read into R afterwards. We will use the function sourced above that was written by a user and is available on GitHub. We will add a case to our function, which executes this function as an alternative whenever an error occurs. Such routing in case of errors can be very helpful, should it be of interest to your, have a look at this [book chapter](http://adv-r.had.co.nz/Exceptions-Debugging.html) by Hadley Wickham.

```{r}
load_and_concatenate_tweets <- function(file_names) {
  
  # Empty list to store the individual dataframes in
  df_list <- list()
  ii <- 1
  
  # Loop over files
  for (ff in file_names) {
  
    # Read in the file with the standard `parse_stream` function, but switch
    # to `recover_stream` if an error occurs
    current_df <- tryCatch({
      
      parse_stream(ff)
      
    }, error = function(e) {print(paste("Retrying with alternative function after initial error when parsing file", ff)); return(recover_stream(ff))})
    
  
  # Store current dataframe in list
  df_list[[ii]] <- current_df
  ii <- ii + 1
  print(paste("Done parsing file", ff))

  }
  
  # Combine individual files
  df <- bind_rows(df_list)

  # Return combined dataframe
  return(df)
  
}
```

Loading all files (these are files I have downloaded via the streaming API before, change accordingly for your application). For large files, this will take considerable time. Note: If an error occurs, we will need to supply some user input in the console which is why this cell cannot be knit.

```{r, eval = FALSE}
# When replicating, change these file names according to what you downloaded with the API
all_file_names <- c("disapproval_emoji_tweets_1.json",
                    "disapproval_emoji_tweets_2.json",
                    "disapproval_emoji_tweets_3.json",
                    "approval_emoji_tweets_1.json",
                    "approval_emoji_tweets_2.json",
                    "general_sample_of_tweets_1.json",
                    "general_sample_of_tweets_2.json")

df_raw <- load_and_concatenate_tweets(file_names = all_file_names)
```

Observations to start with:

```{r}
#df_raw <- read.csv("df_raw.csv") # saved previously when knitting
nrow(df_raw)
```

Keeping only English tweets and determining tweets labels:

```{r}
# Select only text and language column
df <- df_raw %>% select(text, lang)

# Remove all observations that are not English (note that the rweet filter while streaming tweets seems to have worked work imperfectly here)
df <- df[df$lang == "en",]
df$lang <- NULL

# Add counts of the individual classes
df$n_emojis_class_1 <- str_count(df$text, pattern = paste(emojis_class_1, collapse="|"))
df$n_emojis_class_2 <- str_count(df$text, pattern = paste(emojis_class_2, collapse="|"))

# Add labels
df <- df %>% add_column(label = "unknown")
df$label[(df$n_emojis_class_1 == 0) & (df$n_emojis_class_2 == 0)] <- label_0
df$label[(df$n_emojis_class_1 > 0) & (df$n_emojis_class_2 == 0)] <- label_1
df$label[(df$n_emojis_class_1 == 0) & (df$n_emojis_class_2 > 0)] <- label_2

# Keep only the clearly labeled observations
df <- df[df$label %in% c(label_0, label_1, label_2),]
```

Processing the text:

```{r}
# Convert to lower case
df$text <- df$text %>% tolower()

# Remove Twitter handles and hashtags
df$text <- str_replace_all(df$text, pattern = "[@#]\\S+", replacement = "")

# Remove URLs
df$text <- str_replace_all(df$text, pattern = "(http|www)\\S+", replacement = "")

# Get rid of non ASCII characters (largely emojis in this case)
df$text <-gsub("[^\x01-\x7F]", "", df$text)

# Remove punctuation, numbers, and excess white spaces within the texts and at their beginning/end
df$text <- df$text %>% removePunctuation() %>%
    removeNumbers() %>% stripWhitespace() %>% trimws()
```

Removing duplicates and tweets with only one word left:

```{r}
df <- df %>% distinct(text, .keep_all = TRUE)
df <- df[sapply(strsplit(df$text, " "), length) > 1,]
```

Next we will briefly display the most common non stop words. This is a good way to see whether some residual HTML code or very frequent terms e.g. are in the corpus which are not words:

```{r}
crosscheck_dfm <- df$text %>% tokens() %>% tokens_remove(stopwords("en")) %>% dfm()
sort(colSums(crosscheck_dfm), decreasing = TRUE)[1:50]
```


```{r}
df$text <- str_replace_all(df$text, pattern = "amp", replacement = "")
```


Checking how many observation of each class are left:

```{r}
obs_counts <- df %>% group_by(label) %>% summarise(n = n())

n_label_0 <- obs_counts %>% filter(label == label_0) %>% select(n) %>% as.numeric()
n_label_1 <- obs_counts %>% filter(label == label_1) %>% select(n) %>% as.numeric()
n_label_2 <- obs_counts %>% filter(label == label_2) %>% select(n) %>% as.numeric()

obs_counts
```

Sampling classes to an equal amount of observations:

```{r}
set.seed(123)

# Determine smallest class
n_smallest_class <- min(c(n_label_0, n_label_1, n_label_2))

# Create equal samples
df_0 <- sample_n(df %>% filter(label == label_0), n_smallest_class, replace = FALSE)
df_1 <- sample_n(df %>% filter(label == label_1), n_smallest_class, replace = FALSE)
df_2 <- sample_n(df %>% filter(label == label_2), n_smallest_class, replace = FALSE)

# Combine into one dataframe
df_downsampled <- bind_rows(list(df_0, df_1, df_2))

# Shuffle final df
df_downsampled <- sample_n(df_downsampled, nrow(df_downsampled), replace = FALSE)
```

Observations left:

```{r}
nrow(df_downsampled)
```

Writing the final dataframe to disk:

```{r}
write_csv(df_downsampled, "labeled_tweets_processed.csv")
```

